{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56003, 17)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Input Dataset\n",
    "dataset = pd.read_csv('Dataset/HeartDisease.csv')\n",
    "dataset = dataset.values\n",
    "dataset_train = dataset[:46000, :]\n",
    "dataset_test = dataset[46000:, :]\n",
    "X_train = dataset_train[:, 1:]\n",
    "Y_train = dataset_train[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Node class\n",
    "class Node():\n",
    "    def __init__(self, index = None, threshold = None, left = None, right = None, info_gain = None, value = None):\n",
    "        self.index = index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.info_gain = info_gain\n",
    "        self.value = value\n",
    "\n",
    "#Decision Tree class\n",
    "class DecisionTree():\n",
    "    def __init__(self, min_samples = None, max_depth = None):\n",
    "        self.root = None\n",
    "        self.min_samples = min_samples\n",
    "        self.max_depth = max_depth\n",
    "        self.pos = 0\n",
    "\n",
    "    #Return: Node\n",
    "    def build_tree(self, dataset, pos):\n",
    "\n",
    "        X = dataset[:, 1:]\n",
    "        Y = dataset[:, 0]\n",
    "\n",
    "        num_sample, num_feature = np.shape(X)\n",
    "\n",
    "        if num_sample > self.min_samples and pos < self.max_depth:\n",
    "            best_split = self.get_best_split(dataset, num_sample, num_feature)\n",
    "            if best_split[\"info_gain\"] > 0:\n",
    "                self.pos = self.pos + 1\n",
    "                left_subtree = self.build_tree(best_split[\"left_data\"], self.pos)\n",
    "                right_subtree= self.build_tree(best_split[\"right_data\"], self.pos)\n",
    "\n",
    "                return Node(index = best_split[\"index\"], threshold = best_split[\"threshold\"], left = left_subtree, right = right_subtree,info_gain = best_split[\"info_gain\"])\n",
    "\n",
    "        value = self.leaf_value(Y)\n",
    "        return Node(value = value)\n",
    "\n",
    "    #Return a dictionary\n",
    "    def get_best_split(self, dataset, num_sample, num_feature):\n",
    "\n",
    "        best_split = {}\n",
    "        max_info_gain = -999999999999\n",
    "        for index in range (1, num_feature + 1):\n",
    "            threshold_values = dataset[:, index]\n",
    "            unique_values = np.unique(threshold_values)\n",
    "            for value in unique_values:\n",
    "                dataset_x, dataset_y = self.split(dataset, index, value)\n",
    "                if len(dataset_x) > 0 and len(dataset_y) > 0:\n",
    "                  info_gain = self.get_info_gain(dataset, dataset_x, dataset_y)\n",
    "                  if info_gain > max_info_gain:\n",
    "                      max_info_gain = info_gain\n",
    "                      best_split[\"index\"] = index\n",
    "                      best_split[\"threshold\"] = value\n",
    "                      best_split[\"left_data\"] = dataset_x\n",
    "                      best_split[\"right_data\"] = dataset_y\n",
    "                      best_split[\"info_gain\"] = info_gain\n",
    "        return best_split\n",
    "\n",
    "    def split(self, dataset, index, threshold):\n",
    "        dataset_x = np.array([row for row in dataset if row[index] <= threshold])\n",
    "        dataset_y = np.array([row for row in dataset if row[index] > threshold])\n",
    "        return dataset_x, dataset_y\n",
    "\n",
    "    def get_info_gain(self, dataset, dataset_x, dataset_y):\n",
    "        weight_x = len(dataset_x)/len(dataset)\n",
    "        weight_y = len(dataset_y)/len(dataset)\n",
    "        return self.gini_index(dataset) - weight_x*self.gini_index(dataset_x) - weight_y*self.gini_index(dataset_y)\n",
    "\n",
    "    def gini_index(self, dataset):\n",
    "        data_label = dataset[:, 0]\n",
    "        result = 0\n",
    "        labels = np.unique(data_label)\n",
    "        for label in labels:\n",
    "            value = len(data_label[data_label == label ])/len(data_label)\n",
    "            result += value**2\n",
    "\n",
    "        return 1 - result\n",
    "\n",
    "    def leaf_value(self, Y):\n",
    "        Y = list(Y)\n",
    "        return max(Y, key = Y.count)\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        self.root = self.build_tree(dataset, self.pos)\n",
    "\n",
    "    def make_predictions(self, X):\n",
    "        return [self.predict(row) for row in X]\n",
    "\n",
    "    def predict(self, X):\n",
    "        cur_node = self.root\n",
    "        while cur_node.value is None:\n",
    "            if X[cur_node.index - 1] <= cur_node.threshold:\n",
    "                cur_node = cur_node.left\n",
    "            else:\n",
    "                cur_node = cur_node.right\n",
    "        return cur_node.value\n",
    "\n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "\n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        if tree.value is not None:\n",
    "            print(tree.value)\n",
    "\n",
    "        else:\n",
    "            print(\"X_\"+str(tree.index), \"<=\", tree.threshold, \"?\", tree.info_gain)\n",
    "            print(\"%sleft:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.left, indent + indent)\n",
    "            print(\"%sright:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.right, indent + indent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adaboost class\n",
    "class Adaboost():\n",
    "    def __init__(self, nb_model):\n",
    "        self.nb_model = nb_model\n",
    "        self.stumps = []\n",
    "        self.amount_of_says = []\n",
    "    \n",
    "    def fit(self, dataset):\n",
    "        X = dataset[:, 1:]\n",
    "        Y = dataset[:, 0]\n",
    "        num_sample, num_feature = np.shape(dataset)\n",
    "        prob_weight = torch.ones(num_sample)\n",
    "        prob_weight = prob_weight/num_sample\n",
    "        prob_weight = prob_weight.numpy()\n",
    "        count = 0\n",
    "        sample_info = [[w, 0] for w in prob_weight]\n",
    "        weight = None\n",
    "        for i in range(self.nb_model):\n",
    "            if count == 0:\n",
    "                training_data = dataset\n",
    "            else:\n",
    "                training_data = self.create_new_dataset(dataset, weight, num_sample)\n",
    "            stump = DecisionTree(min_samples=13, max_depth=3)\n",
    "            stump.fit(training_data)\n",
    "            predictions = np.array(stump.make_predictions(X))\n",
    "            amount_of_say, result = self.calculate_aos(Y, predictions, num_sample)\n",
    "            self.stumps.append(stump)\n",
    "            self.amount_of_says.append(amount_of_say)\n",
    "            #Update result of sample testing\n",
    "            for j in range(len(sample_info)):\n",
    "                sample_info[j][1] = result[j]\n",
    "            #Update original weight\n",
    "            for index in range(len(result)):\n",
    "                if sample_info[index][1] == 0:\n",
    "                    sample_info[index][0] = self.update_weight(sample_info[index][0], amount_of_say, False)\n",
    "                else:\n",
    "                    sample_info[index][0] = self.update_weight(sample_info[index][0], amount_of_say, True)\n",
    "            weight = self.get_weight(sample_info)\n",
    "            for m in range(num_sample):\n",
    "                sample_info[m][0] = sample_info[m][0]/np.sum(weight)\n",
    "            weight = self.get_weight(sample_info)\n",
    "            count += 1\n",
    "\n",
    "    def get_weight(self, info):\n",
    "        result = []\n",
    "        for i in range(len(info)):\n",
    "            result.append(info[i][0])\n",
    "        return np.array(result)\n",
    "\n",
    "    def calculate_aos(self, Y, predictions, num_sample):\n",
    "        result = predictions == Y\n",
    "        result = result.astype(int)\n",
    "        error = min(1 - (np.sum(result)/num_sample), (np.sum(result)/num_sample))\n",
    "        #error = 1 - (np.sum(result)/num_sample)\n",
    "        amount_of_say = np.log((1-error)/error)/2\n",
    "        return amount_of_say, result\n",
    "    \n",
    "    def update_weight(self, weight, amount_of_say, state_of_prediction):\n",
    "        if state_of_prediction == False:\n",
    "            return weight*np.exp(amount_of_say)\n",
    "        else:\n",
    "            return weight*np.exp(-amount_of_say)\n",
    "        \n",
    "    def create_new_dataset(self, dataset, weight, num_sample):\n",
    "        data_index = []\n",
    "        for count in range(num_sample):\n",
    "            random = np.random.rand()\n",
    "            index = 0\n",
    "            a = weight[0]\n",
    "            while a < random:\n",
    "                index += 1\n",
    "                a += weight[index]\n",
    "            data_index.append(index)\n",
    "        new_dataset = np.stack([dataset[ind] for ind in data_index])\n",
    "        return new_dataset\n",
    "    \n",
    "    def make_prediction(self, X):\n",
    "        return [self.ada_predict(row) for row in X]\n",
    "\n",
    "    def ada_predict(self, X):\n",
    "        aof_true = 0\n",
    "        aof_false = 0\n",
    "        for ind in range(self.nb_model):\n",
    "            if self.stumps[ind].predict(X) == 1:\n",
    "                aof_true += self.amount_of_says[ind]\n",
    "            else:\n",
    "                aof_false += self.amount_of_says[ind]\n",
    "        return aof_true > aof_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model fitting/loading\n",
    "try:\n",
    "    model = pickle.load(open('adaboost_model.pkl', 'rb'))\n",
    "except:\n",
    "    model = Adaboost(30)\n",
    "    model.fit(dataset_train)\n",
    "    filename = 'adaboost_model.pkl'\n",
    "    pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for testing 0.9341197640707788\n"
     ]
    }
   ],
   "source": [
    "#Model Testing\n",
    "X = dataset_test[:, 1:]\n",
    "Y = dataset_test[:, 0]\n",
    "prediction = model.make_prediction(X)\n",
    "bool_result = prediction == Y\n",
    "bool_result = bool_result.astype(int)\n",
    "num_correct = np.sum(bool_result)\n",
    "print(f\"Accuracy for testing {num_correct/np.shape(Y)[0]*100}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
